{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0301201e",
   "metadata": {},
   "source": [
    "# INN Applied to Molecular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300429a",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 SMILES examples:\n",
      "\n",
      "CN(c1ccc(NC(=O)Nc2ccccc2)cc1)S(=O)(=O)c1ccc(-c2ccn(CCO)n2)s1\n",
      "\n",
      "CC(O)CC(C)C#COC#CC(C)CC(C)O\n",
      "\n",
      "Cc1cc([N+](=O)[O-])ccc1NC(=O)c1ccc(OCC(C)C)c(Br)c1\n",
      "\n",
      "COC12C(COC(N)=O)C3=C(C(=O)C(C)=C(N)C3=O)N1CC1NC12\n",
      "\n",
      "CC(C)(O)C(NC(=O)c1cnn2cc(C3CC3)cnc12)c1ccc(OC(F)(F)F)c(F)c1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Draw\n",
    "from rdkit.Chem import rdmolfiles\n",
    "\n",
    "computed_spectra = pd.read_csv(\"/home/undergrad/2026/wcarvalh/Documents/uvsq/\" + \"computed_spectra.csv\")\n",
    "\n",
    "smiles = computed_spectra[\"smiles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a33219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Displaying spectra for 5 SMILES:\\n\")\\nfor i in range(5):\\n    print(f\"SMILES {i+1}: {smiles[i]}\")\\n    print(\"Spectra:\", smiles_spectra[i])\\n    print(\"\\n\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the df. each row corresponds to a smile/molecule, and the columns to the right of the molecule represent its spectra values\n",
    "smiles_spectra = computed_spectra.iloc[:, 1:].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a53b11",
   "metadata": {},
   "source": [
    "### SMILES -> SELFIES\n",
    "Transform SMILES to SELFIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4648d90",
   "metadata": {},
   "source": [
    "### Create the EMBEDDED_SMILES <--> SPECTRA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selfies as sf\n",
    "from selfies import split_selfies\n",
    "\n",
    "class SelfiesEncoder:\n",
    "    def __init__(self, selfies_list, pad_token='[PAD]', max_len=1801):\n",
    "        self.max_len = max_len\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "        # tokenize and build vocabulary\n",
    "        tokenized = [list(split_selfies(s)) for s in selfies_list]\n",
    "        unique_tokens = sorted(set(tok for seq in tokenized for tok in seq))\n",
    "\n",
    "        # create a dictionary mapping each unique token to an integer index\n",
    "        self.token2idx = {tok: i for i, tok in enumerate(unique_tokens)}\n",
    "        self.pad_idx = len(self.token2idx)\n",
    "        self.token2idx[self.pad_token] = self.pad_idx\n",
    "\n",
    "        self.idx2token = {i: tok for tok, i in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, selfies_str):\n",
    "        tokens = list(split_selfies(selfies_str))\n",
    "        token_ids = [self.token2idx[tok] for tok in tokens]\n",
    "        # pad or truncate to max_len\n",
    "        if len(token_ids) < self.max_len:\n",
    "            token_ids += [self.pad_idx] * (self.max_len - len(token_ids))\n",
    "        else:\n",
    "            token_ids = token_ids[:self.max_len]\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.idx2token[i] for i in token_ids if i != self.pad_idx]\n",
    "        return ''.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "268684d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example encoded selfie: [64, 78, 50, 88, 25, 64, 28, 64, 28, 49, 3, 78, 64, 24, 64, 36, 78, 64, 28, 64, 28, 64, 28, 88, 24, 64, 28, 88, 94, 94, 24, 64, 36, 24, 64, 36, 64, 28, 64, 28, 49, 34, 64, 64, 28, 78, 49, 89, 64, 64, 82, 78, 40, 50, 94, 88, 34, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115]\n",
      "Length: 1801\n"
     ]
    }
   ],
   "source": [
    "selfies = []\n",
    "for smile in smiles:\n",
    "    selfies_str = sf.encoder(smile)\n",
    "    selfies.append(selfies_str)\n",
    "\n",
    "encoder = SelfiesEncoder(selfies)\n",
    "\n",
    "embedded_selfies = [encoder.encode(s) for s in selfies]\n",
    "\n",
    "# 4. Check the result\n",
    "print(\"Example encoded selfie:\", embedded_selfies[0])\n",
    "print(\"Length:\", len(embedded_selfies[0]))  # should be 1801\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e3a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example decoded SELFIE: [C][N][Branch2][Ring1][=Branch2][C][=C][C][=C][Branch1][#C][N][C][=Branch1][C][=O][N][C][=C][C][=C][C][=C][Ring1][=Branch1][C][=C][Ring1][S][S][=Branch1][C][=O][=Branch1][C][=O][C][=C][C][=C][Branch1][=N][C][C][=C][N][Branch1][Ring2][C][C][O][N][=Ring1][Branch2][S][Ring1][=N]\n",
      "Same SELFIE as a SMILE: CN(C1=CC=C(NC(=O)NC2=CC=CC=C2)C=C1)S(=O)(=O)C3=CC=C(C=4C=CN(CCO)N=4)S3\n"
     ]
    }
   ],
   "source": [
    "decoded_selfie = encoder.decode(embedded_selfies[0])\n",
    "print(\"Example decoded SELFIE:\", decoded_selfie)\n",
    "print(\"Same SELFIE as a SMILE:\", sf.decoder(decoded_selfie))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47f5ea",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb9889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token_0  token_1  token_2  token_3  token_4  token_5  token_6  token_7  \\\n",
      "0       64       78       50       88       25       64       28       64   \n",
      "1       64       64       49       64       82       64       64       49   \n",
      "2       64       64       28       64       49       24       69       24   \n",
      "3       64       82       64       64       49       50       64       82   \n",
      "4       64       64       49       64       64       49       64       82   \n",
      "\n",
      "   token_8  token_9  ...  spectra_1791  spectra_1792  spectra_1793  \\\n",
      "0       28       49  ...      0.000774      0.000786      0.000803   \n",
      "1       64       64  ...      0.001032      0.001046      0.001065   \n",
      "2       64       36  ...      0.000462      0.000465      0.000453   \n",
      "3       64       49  ...      0.001045      0.001055      0.001057   \n",
      "4       64       50  ...      0.000649      0.000653      0.000655   \n",
      "\n",
      "   spectra_1794  spectra_1795  spectra_1796  spectra_1797  spectra_1798  \\\n",
      "0      0.000780      0.000782      0.000776      0.000770      0.000765   \n",
      "1      0.001040      0.001047      0.001011      0.001003      0.001020   \n",
      "2      0.000481      0.000469      0.000452      0.000458      0.000458   \n",
      "3      0.001063      0.001051      0.001031      0.001030      0.001023   \n",
      "4      0.000656      0.000650      0.000636      0.000633      0.000632   \n",
      "\n",
      "   spectra_1799  spectra_1800  \n",
      "0      0.000764      0.000767  \n",
      "1      0.001040      0.000981  \n",
      "2      0.000465      0.000448  \n",
      "3      0.001026      0.001022  \n",
      "4      0.000635      0.000628  \n",
      "\n",
      "[5 rows x 3602 columns]\n"
     ]
    }
   ],
   "source": [
    "# column 1 = embedded selfie; column 2 = computed spectra\n",
    "\n",
    "# convert computed_spectra to dataframe\n",
    "spectra_df = pd.DataFrame(computed_spectra)\n",
    "\n",
    "# separate SMILES and spectra\n",
    "#smiles_list = spectra_df.iloc[:, 0].tolist()  # First column: SMILES strings\n",
    "spectra_values = spectra_df.iloc[:, 1:].values  # Remaining columns: spectra\n",
    "\n",
    "# create new df\n",
    "final_data = []\n",
    "for embedded, spectra in zip(embedded_selfies, spectra_values):\n",
    "    final_data.append(embedded + list(spectra))\n",
    "\n",
    "# create column names\n",
    "num_tokens = len(embedded_selfies[0])\n",
    "num_spectra = spectra_values.shape[1]\n",
    "columns = [f\"token_{i}\" for i in range(num_tokens)] + [f\"spectra_{i}\" for i in range(num_spectra)]\n",
    "\n",
    "# create final dataframe\n",
    "df = pd.DataFrame(final_data, columns=columns)\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea8e23",
   "metadata": {},
   "source": [
    "## INN setup\n",
    "Forward: f(embedding) -> spectra\n",
    "Inverse: f^-1(spectra) -> embedding\n",
    "\n",
    "Implement normalization later (min-max or z-score)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "691eeb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85506, 1801)\n",
      "(85506, 1801)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_tokens = len([col for col in df.columns if col.startswith(\"token_\")])\n",
    "num_spectra = len([col for col in df.columns if col.startswith(\"spectra_\")])\n",
    "\n",
    "# X = embedded selfies; y = spectra\n",
    "X = df.iloc[:, :num_tokens].values.astype(np.float32)\n",
    "y = df.iloc[:, num_tokens:].values.astype(np.float32)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ccf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class INN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(INN, self).__init__()\n",
    "        hidden = 512\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, output_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def inverse(self, y):\n",
    "        return self.decoder(y)\n",
    "\n",
    "# Initialize model\n",
    "model = INN(input_dim=num_tokens, output_dim=num_spectra)\n",
    "\n",
    "# Step 6: Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7969f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Training loop\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Step 8: Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_test_loss = 0\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        total_test_loss += loss.item()\n",
    "    print(f\"\\nTest Loss: {total_test_loss / len(test_loader):.4f}\")\n",
    "\n",
    "# Optional: Test the inverse (predict SELFIES from spectra)\n",
    "sample_spectra = y_test_tensor[0].unsqueeze(0)  # Add batch dim\n",
    "reconstructed_selfie = model.inverse(sample_spectra)\n",
    "print(\"\\nOriginal embedded selfie:\\n\", X_test_tensor[0].tolist())\n",
    "print(\"Reconstructed selfie embedding:\\n\", reconstructed_selfie.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db396af",
   "metadata": {},
   "source": [
    "### Build and train the INN\n",
    "Using PyTorch's FrEIA (Framework for Easily Invertible Architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import FrEIA.framework as Ff\n",
    "import FrEIA.modules as Fm\n",
    "\n",
    "# Subnet for each block\n",
    "def subnet_fc(dims_in, dims_out):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dims_in, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, dims_out)\n",
    "    )\n",
    "\n",
    "# Simulated data\n",
    "batch_size = 32\n",
    "embedding_dim = 128\n",
    "spectra_dim = 1801\n",
    "\n",
    "# split the data 80x20\n",
    "# x = embeddings, y = spectra\n",
    "# create INN to predict spectra\n",
    "inn = Ff.SequenceINN(embedding_dim)\n",
    "\n",
    "for _ in range(4):\n",
    "    inn.append(Fm.AllInOneBlock,\n",
    "               subnet_constructor=subnet_fc,\n",
    "               permute_soft=True)\n",
    "\n",
    "# Project spectra to match embedding_dim (for inversion)\n",
    "proj = nn.Linear(spectra_dim, embedding_dim)\n",
    "unproj = nn.Linear(embedding_dim, spectra_dim)\n",
    "\n",
    "# === TRAINING EXAMPLE ===\n",
    "\n",
    "# Forward: embedding → spectra\n",
    "z = inn(x_embedding)                   # latent rep\n",
    "output = unproj(z)                     # predicted spectra\n",
    "\n",
    "# Inverse: spectra → embedding\n",
    "z_inv = proj(y_spectra)                # compress spectra\n",
    "x_reconstructed = inn(z_inv, rev=True)\n",
    "\n",
    "# === LOSS ===\n",
    "loss_forward = nn.MSELoss()(output, y_spectra)\n",
    "loss_inverse = nn.MSELoss()(x_reconstructed, x_embedding)\n",
    "\n",
    "print(\"Forward loss:\", loss_forward.item())\n",
    "print(\"Inverse loss:\", loss_inverse.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "optimizer = torch.optim.Adam(list(inn.parameters()) + list(proj.parameters()) + list(unproj.parameters()), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    z = inn(x_embedding)\n",
    "    output = unproj(z)\n",
    "    z_inv = proj(y_spectra)\n",
    "    x_reconstructed = inn(z_inv, rev=True)\n",
    "\n",
    "    loss_fwd = nn.MSELoss()(output, y_spectra)\n",
    "    loss_inv = nn.MSELoss()(x_reconstructed, x_embedding)\n",
    "    loss = loss_fwd + loss_inv\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} | Forward loss: {loss_fwd.item():.4f} | Inverse loss: {loss_inv.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e339e",
   "metadata": {},
   "source": [
    "### Invert the INN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f28e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_pred = inn(spectra_input, rev=True)\n",
    "\n",
    "# compare to known embeddings\n",
    "# use similarity search (MSE?)\n",
    "# return top-k closest SMILES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
