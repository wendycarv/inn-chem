{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    IPython.notebook.save_checkpoint();\n    setInterval(function() {\n        IPython.notebook.save_checkpoint();\n        console.log(\"Auto-saved notebook at \" + new Date().toLocaleTimeString());\n    }, 300000);\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "# Auto-save every 5 minutes\n",
    "display(Javascript('''\n",
    "    IPython.notebook.save_checkpoint();\n",
    "    setInterval(function() {\n",
    "        IPython.notebook.save_checkpoint();\n",
    "        console.log(\"Auto-saved notebook at \" + new Date().toLocaleTimeString());\n",
    "    }, 300000);\n",
    "'''))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selfies as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import selfies as sf\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from cddd.inference import InferenceModel\n",
    "from cddd.preprocessing import preprocess_smiles\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From example/run_qsar_test.py:94: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0724 08:29:30.305327 136673072039424 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"example/run_qsar_test.py\", line 94, in <module>\n",
      "    tf.app.run(main=main, argv=[sys.argv[0]] + UNPARSED)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"example/run_qsar_test.py\", line 51, in main\n",
      "    infer_model = InferenceModel(model_dir, use_gpu=FLAGS.gpu, cpu_threads=FLAGS.cpu_threads)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/cddd/inference.py\", line 102, in __init__\n",
      "    self.hparams = create_hparams(flags)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/cddd/hyperparameters.py\", line 119, in create_hparams\n",
      "    hparams = hparams.parse_json(json.load(open(hparams_file_name)))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'cddd/pretrained_models/default_model/hparams.json'\n"
     ]
    }
   ],
   "source": [
    "!python3 example/run_qsar_test.py --model_dir cddd/pretrained_models/default_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/cddd/run_cddd.py:99: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/cddd/run_cddd.py:55: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  sml_df = pd.read_table(file, header=None).rename({0:FLAGS.smiles_header, 1:\"EXTREG\"},\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/bin/cddd\", line 8, in <module>\n",
      "    sys.exit(main_wrapper())\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/cddd/run_cddd.py\", line 99, in main_wrapper\n",
      "    tf.app.run(main=main, argv=[sys.argv[0]] + UNPARSED)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/cddd/run_cddd.py\", line 67, in main\n",
      "    df = read_input(file)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/cddd/run_cddd.py\", line 55, in read_input\n",
      "    sml_df = pd.read_table(file, header=None).rename({0:FLAGS.smiles_header, 1:\"EXTREG\"},\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\", line 702, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\", line 429, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\", line 895, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1122, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"/home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1853, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 387, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas/_libs/parsers.pyx\", line 705, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: [Errno 2] File b'smiles.smi' does not exist: b'smiles.smi'\n"
     ]
    }
   ],
   "source": [
    "!cddd --input smiles.smi --output descriptors.csv  --smiles_header smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smiles to Embedding via cddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/model_helper.py:42: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/model_helper.py:43: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/model_helper.py:48: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:69: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:827: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:831: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:920: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:921: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:926: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:928: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:856: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/model_helper.py:62: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/france_projects/cddd/cddd/models.py:436: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/undergrad/2026/melkoudi/miniconda3/envs/tf110/lib/python3.6/site-packages/tensorflow_core/contrib/seq2seq/python/ops/beam_search_decoder.py:971: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "created model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import and preprocess dataset\n",
    "\n",
    "input_csv = \"computed_spectra.csv\"\n",
    "output_csv = \"../spectra_and_embeddings_full2.csv\"\n",
    "row_num = 0\n",
    "\n",
    "# create an instance of the model\n",
    "inference_model = InferenceModel()\n",
    "print(\"created model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../spectra_and_embeddings_full2.csv already exists. loading data... \n",
      "spectra shape:  (85506, 1801)\n",
      "embedded smiles shape:  (85506, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# skip preprocessing if file already exists\\nif os.path.exists(output_csv):\\n    print(f\"{output_csv} already exists. loading data... \")\\n\\n    df = pd.read_csv(output_csv)\\n\\n    # Select first 6000 rows\\n    df = df.iloc[:6000]\\n\\n    smiles_list = df[\"smiles\"].tolist()\\n    spectra_array = df.iloc[:, 1:1802].values   # spectra columns\\n    embedded_smiles = df.iloc[:, 1802:].values  # embedded SMILES columns\\n\\n    print(\"spectra shape: \", spectra_array.shape)\\n    print(\"embedded smiles shape: \", embedded_smiles.shape)\\n\\nelse:\\n    print(\"Not here...\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# skip preprocessing if file already exists (full dataset)\n",
    "if os.path.exists(output_csv):\n",
    "    print(f\"{output_csv} already exists. loading data... \")\n",
    "\n",
    "    df = pd.read_csv(output_csv)\n",
    "    smiles_list = df[\"smiles\"].tolist()\n",
    "\n",
    "    spectra_array = df.iloc[:, 1:1802].values   # spectra columns\n",
    "    embedded_smiles = df.iloc[:, 1802:].values  # embedded SMILES columns\n",
    "\n",
    "    print(\"spectra shape: \", spectra_array.shape)\n",
    "    print(\"embedded smiles shape: \", embedded_smiles.shape)\n",
    "\n",
    "else:\n",
    "    print(\"Not here...\")\n",
    "\n",
    "\"\"\"# skip preprocessing if file already exists\n",
    "if os.path.exists(output_csv):\n",
    "    print(f\"{output_csv} already exists. loading data... \")\n",
    "\n",
    "    df = pd.read_csv(output_csv)\n",
    "\n",
    "    # Select first 6000 rows\n",
    "    df = df.iloc[:6000]\n",
    "\n",
    "    smiles_list = df[\"smiles\"].tolist()\n",
    "    spectra_array = df.iloc[:, 1:1802].values   # spectra columns\n",
    "    embedded_smiles = df.iloc[:, 1802:].values  # embedded SMILES columns\n",
    "\n",
    "    print(\"spectra shape: \", spectra_array.shape)\n",
    "    print(\"embedded smiles shape: \", embedded_smiles.shape)\n",
    "\n",
    "else:\n",
    "    print(\"Not here...\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# create a smiles list of 6000\\nsmiles_list = []\\n\\nwith open(\"computed_spectra.csv\", \"r\") as f:\\n    reader = csv.reader(f)\\n    next(reader)  # skip header line\\n    for i, row in enumerate(reader):\\n        if i >= 6000:\\n            break\\n        smiles = row[0].strip()\\n        try:\\n            smiles_list.append(smiles)\\n        except:\\n            continue\\n\\n# make instance of autoencoder model\\ninference_model = InferenceModel(model_dir=\"cddd/data/default_model\")\\n\\n# embed the smiles\\n# get 512-dim CDDD embeddings\\nsmiles_embedding = inference_model.seq_to_emb(smiles_list)\\n\\n\\n# print example to see if it works\\nfor i in range(5):\\n    print(f\"SMILES: {smiles_list[i]}\") # full smiles\\n    print(f\"Embedding: {smiles_embedding[i][:10]}...\\n\")  # print first 10 values of the embedding'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# create a smiles list of 6000\n",
    "smiles_list = []\n",
    "\n",
    "with open(\"computed_spectra.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # skip header line\n",
    "    for i, row in enumerate(reader):\n",
    "        if i >= 6000:\n",
    "            break\n",
    "        smiles = row[0].strip()\n",
    "        try:\n",
    "            smiles_list.append(smiles)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# make instance of autoencoder model\n",
    "inference_model = InferenceModel(model_dir=\"cddd/data/default_model\")\n",
    "\n",
    "# embed the smiles\n",
    "# get 512-dim CDDD embeddings\n",
    "smiles_embedding = inference_model.seq_to_emb(smiles_list)\n",
    "\n",
    "\n",
    "# print example to see if it works\n",
    "for i in range(5):\n",
    "    print(f\"SMILES: {smiles_list[i]}\") # full smiles\n",
    "    print(f\"Embedding: {smiles_embedding[i][:10]}...\\n\")  # print first 10 values of the embedding\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Spectra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized spectra shape: (85506, 1801)\n",
      "Sample normalized spectrum: [0.0086403  0.00853724 0.00871799 0.00868892 0.00868103 0.00868275\n",
      " 0.00859874 0.00842945 0.00839367 0.00825264]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# 6000 only\\nimport numpy as np\\nfrom sklearn.preprocessing import MinMaxScaler\\nimport pandas as pd\\n\\n# Load the full dataframe\\ndf = pd.read_csv(\"../spectra_and_embeddings_full2.csv\")\\n\\n# Take only the first 6000 rows\\ndf_subset = df.iloc[:6000]\\n\\n# Extract the spectra portion (columns 1 to 1801)\\nspectra_array = df_subset.iloc[:, 1:1802].values  # shape: (6000, 1801)\\n\\n# Normalize using MinMaxScaler\\nscaler = MinMaxScaler()\\nnormalized_spectra = scaler.fit_transform(spectra_array)\\n\\n# Final input for the model\\nX = normalized_spectra\\n\\n# Check\\nprint(\"Normalized spectra shape:\", X.shape)\\nprint(\"Sample normalized spectrum:\", X[0][:10])'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full data set\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the full dataframe \n",
    "df = pd.read_csv(\"../spectra_and_embeddings_full2.csv\")\n",
    "\n",
    "# Extract the spectra portion (columns 1 to 1801)\n",
    "spectra_array = df.iloc[:, 1:1802].values  # shape: (85506, 1801)\n",
    "\n",
    "# Normalize using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_spectra = scaler.fit_transform(spectra_array)\n",
    "\n",
    "# Final input for the model\n",
    "X = normalized_spectra\n",
    "\n",
    "# Check\n",
    "print(\"Normalized spectra shape:\", X.shape)\n",
    "print(\"Sample normalized spectrum:\", X[0][:10])\n",
    "\n",
    "\n",
    "\"\"\"# 6000 only\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the full dataframe\n",
    "df = pd.read_csv(\"../spectra_and_embeddings_full2.csv\")\n",
    "\n",
    "# Take only the first 6000 rows\n",
    "df_subset = df.iloc[:6000]\n",
    "\n",
    "# Extract the spectra portion (columns 1 to 1801)\n",
    "spectra_array = df_subset.iloc[:, 1:1802].values  # shape: (6000, 1801)\n",
    "\n",
    "# Normalize using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_spectra = scaler.fit_transform(spectra_array)\n",
    "\n",
    "# Final input for the model\n",
    "X = normalized_spectra\n",
    "\n",
    "# Check\n",
    "print(\"Normalized spectra shape:\", X.shape)\n",
    "print(\"Sample normalized spectrum:\", X[0][:10])\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized spectra shape: (3568, 1801)\n",
      "Sample normalized spectrum: [0.10808033 0.10908509 0.1097319  0.11094673 0.11241841 0.11278684\n",
      " 0.11332587 0.11278429 0.11410056 0.11418134]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "spec_len = 1801\n",
    "spectra_list = []\n",
    "\n",
    "# Load spectra from CSV\n",
    "with open(\"computed_spectra.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # skip header\n",
    "    for i, row in enumerate(reader):\n",
    "        if i >= 6000:\n",
    "            break\n",
    "        try:\n",
    "            spectrum = [float(x) for x in row[1:]]\n",
    "            if len(spectrum) == spec_len:\n",
    "                spectra_list.append(spectrum)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# convert to np array\n",
    "spectra_array = np.array(spectra_list)\n",
    "\n",
    "# normalize using minmax scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_spectra = scaler.fit_transform(spectra_array)\n",
    "\n",
    "# the input for the model\n",
    "X = normalized_spectra\n",
    "\n",
    "# check if it worked :)))\n",
    "print(\"Normalized spectra shape:\", X.shape)\n",
    "print(\"Sample normalized spectrum:\", X[0][:10])\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 68404 Test size: 17102\n",
      "X shape: (85506, 1801) Y shape: (85506, 512)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assume X and Y already aligned and same length\n",
    "X = np.array(normalized_spectra)       # shape (85506, 1801)\n",
    "Y = np.array(embedded_smiles)          # shape (85506, 512)\n",
    "\n",
    "# optional check\n",
    "assert len(X) == len(Y), \"X and Y must have the same number of samples\"\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\n",
    "print(\"X shape:\", X.shape, \"Y shape:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape: (3568, 1801), Y shape: (3568, 512)\n",
      "Train size: 2854 Test size: 714\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set up X and Y\n",
    "X = np.array(normalized_spectra)\n",
    "Y = np.array(smiles_embedding)\n",
    "\n",
    "# fit to right length\n",
    "min_len = min(len(X), len(Y))\n",
    "X = X[:min_len]\n",
    "Y = Y[:min_len]\n",
    "\n",
    "# debug rqqq\n",
    "print(f\" shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SpectraToSMILESDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# wrap train/test sets\n",
    "train_dataset = SpectraToSMILESDataset(X_train, Y_train)\n",
    "test_dataset = SpectraToSMILESDataset(X_test, Y_test)\n",
    "\n",
    "# loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch.nn as nn\n",
    "\n",
    "class SpectraToEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim=1801, output_dim=512, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = SpectraToEmbedding()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpectraToEmbedding(nn.Module):\n",
    "    def __init__(self, input_dim=1801, output_dim=512, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = SpectraToEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train za Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.2445, LR: 0.001000\n",
      "Epoch 2/500, Loss: 0.2390, LR: 0.001000\n",
      "Epoch 3/500, Loss: 0.2340, LR: 0.001000\n",
      "Epoch 4/500, Loss: 0.2301, LR: 0.001000\n",
      "Epoch 5/500, Loss: 0.2271, LR: 0.001000\n",
      "Epoch 6/500, Loss: 0.2242, LR: 0.001000\n",
      "Epoch 7/500, Loss: 0.2215, LR: 0.001000\n",
      "Epoch 8/500, Loss: 0.2191, LR: 0.001000\n",
      "Epoch 9/500, Loss: 0.2169, LR: 0.001000\n",
      "Epoch 10/500, Loss: 0.2147, LR: 0.001000\n",
      "Epoch 11/500, Loss: 0.2127, LR: 0.001000\n",
      "Epoch 12/500, Loss: 0.2112, LR: 0.001000\n",
      "Epoch 13/500, Loss: 0.2095, LR: 0.001000\n",
      "Epoch 14/500, Loss: 0.2079, LR: 0.001000\n",
      "Epoch 15/500, Loss: 0.2065, LR: 0.001000\n",
      "Epoch 16/500, Loss: 0.2053, LR: 0.001000\n",
      "Epoch 17/500, Loss: 0.2038, LR: 0.001000\n",
      "Epoch 18/500, Loss: 0.2028, LR: 0.001000\n",
      "Epoch 19/500, Loss: 0.2016, LR: 0.001000\n",
      "Epoch 20/500, Loss: 0.2007, LR: 0.001000\n",
      "Epoch 21/500, Loss: 0.1996, LR: 0.001000\n",
      "Epoch 22/500, Loss: 0.1987, LR: 0.001000\n",
      "Epoch 23/500, Loss: 0.1978, LR: 0.001000\n",
      "Epoch 24/500, Loss: 0.1968, LR: 0.001000\n",
      "Epoch 25/500, Loss: 0.1961, LR: 0.001000\n",
      "Epoch 26/500, Loss: 0.1954, LR: 0.001000\n",
      "Epoch 27/500, Loss: 0.1945, LR: 0.001000\n",
      "Epoch 28/500, Loss: 0.1939, LR: 0.001000\n",
      "Epoch 29/500, Loss: 0.1933, LR: 0.001000\n",
      "Epoch 30/500, Loss: 0.1928, LR: 0.001000\n",
      "Epoch 31/500, Loss: 0.1919, LR: 0.001000\n",
      "Epoch 32/500, Loss: 0.1910, LR: 0.001000\n",
      "Epoch 33/500, Loss: 0.1905, LR: 0.001000\n",
      "Epoch 34/500, Loss: 0.1901, LR: 0.001000\n",
      "Epoch 35/500, Loss: 0.1895, LR: 0.001000\n",
      "Epoch 36/500, Loss: 0.1891, LR: 0.001000\n",
      "Epoch 37/500, Loss: 0.1883, LR: 0.001000\n",
      "Epoch 38/500, Loss: 0.1877, LR: 0.001000\n",
      "Epoch 39/500, Loss: 0.1876, LR: 0.001000\n",
      "Epoch 40/500, Loss: 0.1870, LR: 0.001000\n",
      "Epoch 41/500, Loss: 0.1865, LR: 0.001000\n",
      "Epoch 42/500, Loss: 0.1861, LR: 0.001000\n",
      "Epoch 43/500, Loss: 0.1855, LR: 0.001000\n",
      "Epoch 44/500, Loss: 0.1852, LR: 0.001000\n",
      "Epoch 45/500, Loss: 0.1848, LR: 0.001000\n",
      "Epoch 46/500, Loss: 0.1845, LR: 0.001000\n",
      "Epoch 47/500, Loss: 0.1839, LR: 0.001000\n",
      "Epoch 48/500, Loss: 0.1835, LR: 0.001000\n",
      "Epoch 49/500, Loss: 0.1832, LR: 0.001000\n",
      "Epoch 50/500, Loss: 0.1829, LR: 0.000500\n",
      "Epoch 51/500, Loss: 0.1787, LR: 0.000500\n",
      "Epoch 52/500, Loss: 0.1777, LR: 0.000500\n",
      "Epoch 53/500, Loss: 0.1773, LR: 0.000500\n",
      "Epoch 54/500, Loss: 0.1768, LR: 0.000500\n",
      "Epoch 55/500, Loss: 0.1767, LR: 0.000500\n",
      "Epoch 56/500, Loss: 0.1762, LR: 0.000500\n",
      "Epoch 57/500, Loss: 0.1760, LR: 0.000500\n",
      "Epoch 58/500, Loss: 0.1757, LR: 0.000500\n",
      "Epoch 59/500, Loss: 0.1755, LR: 0.000500\n",
      "Epoch 60/500, Loss: 0.1751, LR: 0.000500\n",
      "Epoch 61/500, Loss: 0.1753, LR: 0.000500\n",
      "Epoch 62/500, Loss: 0.1750, LR: 0.000500\n",
      "Epoch 63/500, Loss: 0.1746, LR: 0.000500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# full dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 500\n",
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(X_batch)\n",
    "\n",
    "        target = torch.ones(X_batch.size(0)).to(X_batch.device)\n",
    "        loss = loss_fn(Y_pred, Y_batch, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    if avg_loss < best_loss - 1e-4:\n",
    "        best_loss = avg_loss\n",
    "        counter = 0\n",
    "        # Save best model so far\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Final save just in case\n",
    "torch.save(model.state_dict(), \"final_model.pt\")\n",
    "print(\"Model saved to 'final_model.pt'\")\n",
    "\"\"\"\n",
    "# not full data set\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 500\n",
    "best_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = model(X_batch)\n",
    "\n",
    "        target = torch.ones(X_batch.size(0)).to(X_batch.device)\n",
    "        loss = loss_fn(Y_pred, Y_batch, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    if avg_loss < best_loss - 1e-4:\n",
    "        best_loss = avg_loss\n",
    "        counter = 0\n",
    "        # Save best model so far\n",
    "        torch.save(model.state_dict(), \"best_model_full.pt\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Final save just in case\n",
    "torch.save(model.state_dict(), \"final_model_full.pt\")\n",
    "print(\"Model saved to 'final_model_full.pt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from cddd/data/default_model/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from cddd/data/default_model/model.ckpt\n",
      "\n",
      "First prediction:\n",
      "Predicted: CC1=CC(C)(C)NC1C(=O)c1cccc2c(C)ccnc12\n",
      "True:      CCN(Cc1nc2ccccc2c(=O)[nH]1)C(=O)C1C(C=C(C)C)C1(C)C\n",
      "Average Tanimoto Similarity: 0.2200\n",
      "Valid SMILES: 1149/1200 (95.75%)\n",
      "Exact Matches: 1/1200 (0.08%)\n",
      "Saving to directory: /home/undergrad/2026/melkoudi/france_projects/cddd\n",
      "âœ… Saved predicted vs. true SMILES to '/home/undergrad/2026/melkoudi/france_projects/cddd/smiles_predictions_1400.csv'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# full dataset\n",
    "# remove any warnings\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "import pandas as pd\n",
    "import os\n",
    "from cddd.inference import InferenceModel\n",
    "\n",
    "# Instantiate model and load weights\n",
    "model = SpectraToEmbedding()\n",
    "model.load_state_dict(torch.load(\"NNFinal_model.pt\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "\n",
    "# Predict from test_loader\n",
    "predicted_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        preds = model(X_batch)\n",
    "        predicted_embeddings.append(preds)\n",
    "\n",
    "predicted_embeddings = torch.cat(predicted_embeddings, dim=0).cpu().numpy()\n",
    "\n",
    "# Decode embeddings\n",
    "cddd = InferenceModel(model_dir=\"cddd/data/default_model\")\n",
    "pred_smiles = cddd.emb_to_seq(predicted_embeddings)\n",
    "true_smiles = cddd.emb_to_seq(Y_test)\n",
    "\n",
    "# Evaluate\n",
    "tanimoto_scores = []\n",
    "valid_count = 0\n",
    "exact_match_count = 0\n",
    "total = len(true_smiles)\n",
    "\n",
    "for pred, true in zip(pred_smiles, true_smiles):\n",
    "    try:\n",
    "        pred_mol = Chem.MolFromSmiles(pred)\n",
    "        true_mol = Chem.MolFromSmiles(true)\n",
    "\n",
    "        if pred_mol is not None:\n",
    "            valid_count += 1\n",
    "\n",
    "        if pred == true:\n",
    "            exact_match_count += 1\n",
    "\n",
    "        if pred_mol is not None and true_mol is not None:\n",
    "            fp_pred = AllChem.GetMorganFingerprintAsBitVect(pred_mol, 2, nBits=2048)\n",
    "            fp_true = AllChem.GetMorganFingerprintAsBitVect(true_mol, 2, nBits=2048)\n",
    "            similarity = DataStructs.TanimotoSimilarity(fp_pred, fp_true)\n",
    "            tanimoto_scores.append(similarity)\n",
    "        else:\n",
    "            tanimoto_scores.append(0.0)\n",
    "    except:\n",
    "        tanimoto_scores.append(0.0)\n",
    "\n",
    "# Print metrics\n",
    "valid_percent = 100 * valid_count / total\n",
    "exact_match_percent = 100 * exact_match_count / total\n",
    "avg_tanimoto = np.mean(tanimoto_scores)\n",
    "\n",
    "print(f\"\\nFirst prediction:\\nPredicted: {pred_smiles[0]}\\nTrue:      {true_smiles[0]}\")\n",
    "print(f\"Average Tanimoto Similarity: {avg_tanimoto:.4f}\")\n",
    "print(f\"Valid SMILES: {valid_count}/{total} ({valid_percent:.2f}%)\")\n",
    "print(f\"Exact Matches: {exact_match_count}/{total} ({exact_match_percent:.2f}%)\")\n",
    "\n",
    "# Save predictions\n",
    "print(\"Saving to directory:\", os.getcwd())\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"True_SMILES\": true_smiles,\n",
    "    \"Pred_SMILES\": pred_smiles\n",
    "})\n",
    "\n",
    "try:\n",
    "    save_path = os.path.join(os.getcwd(), \"smiles_predictions.csv\")\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved predicted vs. true SMILES to '{save_path}'\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save CSV:\", e)\n",
    "\"\"\"\n",
    "\n",
    "# short dataset\n",
    "\n",
    "# full dataset\n",
    "# remove any warnings\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "import pandas as pd\n",
    "import os\n",
    "from cddd.inference import InferenceModel\n",
    "\n",
    "# Instantiate model and load weights\n",
    "model = SpectraToEmbedding()\n",
    "model.load_state_dict(torch.load(\"final_model_full.pt\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "\n",
    "# Predict from test_loader\n",
    "predicted_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        preds = model(X_batch)\n",
    "        predicted_embeddings.append(preds)\n",
    "\n",
    "predicted_embeddings = torch.cat(predicted_embeddings, dim=0).cpu().numpy()\n",
    "\n",
    "# Decode embeddings\n",
    "cddd = InferenceModel(model_dir=\"cddd/data/default_model\")\n",
    "pred_smiles = cddd.emb_to_seq(predicted_embeddings)\n",
    "true_smiles = cddd.emb_to_seq(Y_test)\n",
    "\n",
    "# Evaluate\n",
    "tanimoto_scores = []\n",
    "valid_count = 0\n",
    "exact_match_count = 0\n",
    "total = len(true_smiles)\n",
    "\n",
    "for pred, true in zip(pred_smiles, true_smiles):\n",
    "    try:\n",
    "        pred_mol = Chem.MolFromSmiles(pred)\n",
    "        true_mol = Chem.MolFromSmiles(true)\n",
    "\n",
    "        if pred_mol is not None:\n",
    "            valid_count += 1\n",
    "\n",
    "        if pred == true:\n",
    "            exact_match_count += 1\n",
    "\n",
    "        if pred_mol is not None and true_mol is not None:\n",
    "            fp_pred = AllChem.GetMorganFingerprintAsBitVect(pred_mol, 2, nBits=2048)\n",
    "            fp_true = AllChem.GetMorganFingerprintAsBitVect(true_mol, 2, nBits=2048)\n",
    "            similarity = DataStructs.TanimotoSimilarity(fp_pred, fp_true)\n",
    "            tanimoto_scores.append(similarity)\n",
    "        else:\n",
    "            tanimoto_scores.append(0.0)\n",
    "    except:\n",
    "        tanimoto_scores.append(0.0)\n",
    "\n",
    "# Print metrics\n",
    "valid_percent = 100 * valid_count / total\n",
    "exact_match_percent = 100 * exact_match_count / total\n",
    "avg_tanimoto = np.mean(tanimoto_scores)\n",
    "\n",
    "print(f\"\\nFirst prediction:\\nPredicted: {pred_smiles[0]}\\nTrue:      {true_smiles[0]}\")\n",
    "print(f\"Average Tanimoto Similarity: {avg_tanimoto:.4f}\")\n",
    "print(f\"Valid SMILES: {valid_count}/{total} ({valid_percent:.2f}%)\")\n",
    "print(f\"Exact Matches: {exact_match_count}/{total} ({exact_match_percent:.2f}%)\")\n",
    "\n",
    "# Save predictions\n",
    "print(\"Saving to directory:\", os.getcwd())\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"True_SMILES\": true_smiles,\n",
    "    \"Pred_SMILES\": pred_smiles\n",
    "})\n",
    "\n",
    "try:\n",
    "    save_path = os.path.join(os.getcwd(), \"smiles_predictions_full2.csv\")\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved predicted vs. true SMILES to '{save_path}'\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to save CSV:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
