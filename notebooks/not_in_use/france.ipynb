{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selfies as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from rdkit import Chem\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import selfies as sf\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding for SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " checklength: 1801\n",
      "sirst encoding as token indices:\n",
      "[34 43 28 ...  0  0  0]\n",
      "\n",
      "Encoding Test\n",
      "OG SMILES:      CN(c1ccc(NC(=O)Nc2ccccc2)cc1)S(=O)(=O)c1ccc(-c2ccn(CCO)n2)s1\n",
      "OG Tokens:      ['[C]', '[N]', '[Branch2]', '[Ring1]', '[=Branch2]', '[C]', '[=C]', '[C]', '[=C]', '[Branch1]', '[#C]', '[N]', '[C]', '[=Branch1]', '[C]', '[=O]', '[N]', '[C]', '[=C]', '[C]', '[=C]', '[C]', '[=C]', '[Ring1]', '[=Branch1]', '[C]', '[=C]', '[Ring1]', '[S]', '[S]', '[=Branch1]', '[C]', '[=O]', '[=Branch1]', '[C]', '[=O]', '[C]', '[=C]', '[C]', '[=C]', '[Branch1]', '[=N]', '[C]', '[C]', '[=C]', '[N]', '[Branch1]', '[Ring2]', '[C]', '[C]', '[O]', '[N]', '[=Ring1]', '[Branch2]', '[S]', '[Ring1]', '[=N]']\n",
      "Token Indices:        [34 43 28 50 14 34 15 34 15 27  6 43 34 13 34 21 43 34 15 34 15 34 15 50\n",
      " 13 34 15 50 52 52 13 34 21 13 34 21 34 15 34 15 27 20 34 34 15 43 27 51\n",
      " 34 34 46 43 23 28 52 50 20]\n",
      "Reconstructed SMILES: CN(C1=CC=C(NC(=O)NC2=CC=CC=C2)C=C1)S(=O)(=O)C3=CC=C(C=4C=CN(CCO)N=4)S3\n"
     ]
    }
   ],
   "source": [
    "token_counter = Counter()\n",
    "num_processed = 0\n",
    "first_10_encodings = []\n",
    "\n",
    "with open(\"computed_spectra.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        smiles = row[0].strip()\n",
    "        try:\n",
    "            selfies_str = sf.encoder(smiles)\n",
    "            tokens = list(sf.split_selfies(selfies_str))\n",
    "            token_counter.update(tokens)\n",
    "\n",
    "            # put encodings in a list to print\n",
    "            if len(first_10_encodings) < 10:\n",
    "                first_10_encodings.append((smiles, selfies_str, tokens))\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        num_processed += 1\n",
    "        if num_processed % 10000 == 0:\n",
    "            print(f\"Processed {num_processed} molecules\")\n",
    "\n",
    "# test to see if this actually works\n",
    "#print(\"\\n encodings:\")\n",
    "#for i, (smiles, selfies_str, tokens) in enumerate(first_10_encodings, 1):\n",
    "#    print(f\"{i}. SMILES: {smiles}\")\n",
    "#    print(f\"   SELFIES: {selfies_str}\")\n",
    "#    print(f\"   Tokens: {tokens}\")\n",
    "\n",
    "# create the vocab\n",
    "special_tokens = ['[PAD]', '[SOS]', '[EOS]', '[UNK]']\n",
    "unique_tokens = list(token_counter.keys())\n",
    "vocab = special_tokens + sorted(unique_tokens)\n",
    "\n",
    "token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "\n",
    "# create tokenized encoding:\n",
    "def encode_tokens(tokens, token2idx):\n",
    "    return [token2idx.get(token, token2idx['[UNK]']) for token in tokens]\n",
    "\n",
    "max_len = 1801  # should be the same size as the spectra\n",
    "encoded_sequences = []  # input data\n",
    "\n",
    "with open(\"computed_spectra.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        smiles = row[0].strip()\n",
    "        try:\n",
    "            selfies_str = sf.encoder(smiles)\n",
    "            tokens = list(sf.split_selfies(selfies_str))\n",
    "            token_ids = encode_tokens(tokens, token2idx)\n",
    "            encoded_sequences.append(token_ids)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# pad to fixed length\n",
    "padded_sequences = pad_sequences(encoded_sequences, maxlen=max_len, padding='post', value=token2idx['[PAD]'])\n",
    "\n",
    "print(f\"\\n checklength: {padded_sequences.shape[1]}\")  # should be 1801\n",
    "\n",
    "# lets see if it works\n",
    "print(\"sirst encoding as token indices:\")\n",
    "print(padded_sequences[0])\n",
    "\n",
    "def decode_vector_to_smiles(vector, idx2token):\n",
    "    # convert indices to tokens\n",
    "    tokens = [idx2token[idx] for idx in vector if idx2token[idx] != '[PAD]']\n",
    "\n",
    "    # joins tokens into a SELFIES string\n",
    "    selfies_str = ''.join(tokens)\n",
    "\n",
    "    # decode SELFIES â†’ SMILES\n",
    "    try:\n",
    "        smiles = sf.decoder(selfies_str)\n",
    "        return smiles\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "#first 5 tokenized & padded sequences\n",
    "#print(\"\\nTokenized and padded sequences (first 5):\")\n",
    "#for i in range(5):\n",
    "#    print(f\"{i+1}: {padded_sequences[i]}\")\n",
    "\n",
    "# make sure we actually got valid encodings (slay)\n",
    "if len(first_10_encodings) > 0:\n",
    "    # OG data\n",
    "    original_smiles = first_10_encodings[0][0]\n",
    "    original_tokens = first_10_encodings[0][2]\n",
    "    encoded_vector = padded_sequences[0]\n",
    "\n",
    "    # decode back to SMILES\n",
    "    reconstructed_smiles = decode_vector_to_smiles(encoded_vector, idx2token)\n",
    "\n",
    "    # test results\n",
    "    print(\"\\nEncoding Test\")\n",
    "    print(\"OG SMILES:     \", original_smiles)\n",
    "    print(\"OG Tokens:     \", original_tokens)\n",
    "    print(\"Token Indices:       \", encoded_vector[:len(original_tokens)])  # this unpads it\n",
    "    print(\"Reconstructed SMILES:\", reconstructed_smiles)\n",
    "else:\n",
    "    print(\"No valid SMILES womp womp...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 encoded tokens: [34 43 28 50 14 34 15 34 15 27]\n",
      "First 10 spectra values: [0.0007712866294600632, 0.0007723908046661789, 0.000774465837466059, 0.0007847973157695205, 0.0007924847952294243, 0.0007974891704944533, 0.0007999480379269527, 0.0008086159723132459, 0.0008120453741895288, 0.0008153232389547017]\n",
      "Length of encoded_smiles vector: 1801\n",
      "Length of spectra vector: 1801\n",
      "Full encoded_smiles vector: [34 43 28 ...  0  0  0]\n",
      "Full spectra vector: [0.0007712866294600632, 0.0007723908046661789, 0.000774465837466059, 0.0007847973157695205, 0.0007924847952294243, 0.0007974891704944533, 0.0007999480379269527, 0.0008086159723132459, 0.0008120453741895288, 0.0008153232389547017, 0.0008237638508585554, 0.0008345718056421103, 0.0008496019539248808, 0.0008605721963095523, 0.0008729362666560199, 0.0008860055004255269, 0.0008934067042321944, 0.0009166259796892721, 0.0009311179264626959, 0.0009422418349666756, 0.0009607422498414929, 0.000969887143792212, 0.000979747922820859, 0.000995961108565514, 0.001007887431099005, 0.0010230757724344665, 0.0012266923231774197, 0.0013788974496784952, 0.0015497319772529105, 0.0014988571925881294, 0.0015327500796578843, 0.001586643110820559, 0.00162081663144077, 0.0016357083179932225, 0.0016557734784997896, 0.0016761161116106628, 0.0016951539236586904, 0.0017272185310544873, 0.001738228295587392, 0.001762239245350857, 0.001783081127191374, 0.0018001281034722252, 0.0018486966915929246, 0.0018724055721559749, 0.0019103645803626313, 0.0019300404325766143, 0.0019547539818225304, 0.0019749763376185644, 0.001997553685084153, 0.002005854912854127, 0.0020062316320278847, 0.0020160172302437426, 0.002024044705882861, 0.0020368642123684863, 0.00204712750423611, 0.0020765368460941777, 0.0021305293420900056, 0.002174790299585725, 0.002205118762263052, 0.002236806035498877, 0.002271935702460046, 0.0022617411122696465, 0.0022822822276592734, 0.0023164509107178534, 0.002345874134373291, 0.002415640843779959, 0.0024080433205193106, 0.0024650430519312853, 0.0025191162832224573, 0.002553166288955165, 0.0025533878781190066, 0.002538384346890648, 0.0026596658676759084, 0.0025957004012281265, 0.0025817189474568363, 0.0026072067292362013, 0.002725382084230696, 0.00256909894488758, 0.0024481202009397915, 0.0023902365031808204, 0.002372979034704055, 0.00238446690213048, 0.002348179870320936, 0.002310166087087967, 0.0022928263506857044, 0.0023020447428103124, 0.002275854937796146, 0.0022505810350227132, 0.002217852737421272, 0.0021633287968222657, 0.002104018591082783, 0.002036041215500835, 0.0019499955090859522, 0.0018584571374134326, 0.0017599878431554042, 0.0016547655898130002, 0.001576116233712326, 0.0015061416608765637, 0.0014317619946676474, 0.0013940328997019262, 0.001347676901899924, 0.001317478177305371, 0.0013012743351990402, 0.0012597880780545728, 0.0012173330069153083, 0.001180151158381924, 0.0011537367117760886, 0.0011253788031268524, 0.0011022276523022503, 0.0011122946354200141, 0.001119478041137313, 0.0011254219981468249, 0.001149542118618375, 0.0011706229902728655, 0.001214273160196414, 0.0012423951651811815, 0.0013026267008240673, 0.0013534441092759417, 0.0014103307523352176, 0.0015010222202741371, 0.0015520528715543033, 0.0016302273991978511, 0.0016698176060064753, 0.0017463512632176015, 0.0018069559070799026, 0.00186848470064992, 0.001925448796906973, 0.0019766641921157174, 0.0020194271618950454, 0.0020552169043401504, 0.0020749793298223563, 0.002078264621737869, 0.002084811325414604, 0.0020952362861440172, 0.0020931621493056126, 0.0020856800137534414, 0.002082207642636692, 0.0020878269832260063, 0.00203525351881744, 0.0020257457215076633, 0.0019933877320473855, 0.001961943180096225, 0.001918350851682476, 0.0018464541825777736, 0.0017941023318588502, 0.0017407218090871232, 0.001677879195895864, 0.0015784092830535353, 0.001504458182089101, 0.0014316799912889855, 0.0013893935938634528, 0.0013072213466202918, 0.001225848411602638, 0.0011492799730336573, 0.0010731730767117712, 0.0010090371329117339, 0.0008981685261337765, 0.0009020965287400636, 0.0008627144108019989, 0.0008254488761861568, 0.0007805564676019763, 0.0007479791614811673, 0.0007471567300890974, 0.0007345153371929257, 0.0007160447160486437, 0.0006764820481516909, 0.0006436474160197029, 0.00063128381070464, 0.0006343206915935456, 0.0006340476417667471, 0.00063623147474719, 0.0006395803147722632, 0.0006566358809027805, 0.0007077555020441009, 0.0007617443294385164, 0.0008178482875819161, 0.0008757184067427434, 0.0009261173590644981, 0.000952804955006523, 0.0009526858660106837, 0.0009654586803398193, 0.0009576919041450113, 0.0009417890038278172, 0.0009131200316925492, 0.0008914049088921159, 0.000883725037866347, 0.0008729041742147384, 0.0008782857442687405, 0.0008822755551121244, 0.0009068259725824837, 0.0009483320814616035, 0.000991145103770136, 0.0010230032942579073, 0.0010712824478150617, 0.0011058869047328916, 0.0011546127031886214, 0.001234007445064877, 0.0012788937066556703, 0.0013219143167396282, 0.0013514841125311361, 0.0013724155178519086, 0.0013869479152805605, 0.0014349290921560267, 0.0014448953418991924, 0.0013940284755502793, 0.0014268679479983084, 0.0014120900370407105, 0.0013797996871867213, 0.0013328893664135465, 0.001294493665332257, 0.0012513603762996587, 0.001192669039136147, 0.0011494613926799572, 0.0011204394786602173, 0.001079006437242113, 0.0010699125685921412, 0.0010617402018383941, 0.001060746601549221, 0.001079577412309171, 0.0011085465660547109, 0.001124385259948843, 0.0011349982035227437, 0.0011518740149471895, 0.0011659599256293447, 0.0011666267453040444, 0.0011447935088336272, 0.0011492424843537366, 0.0011269522884470378, 0.001103013679789269, 0.0010696700724135554, 0.001033404673612021, 0.0010157584238305363, 0.0010028033052757207, 0.0009810107979451772, 0.0009742303802220915, 0.000990813007329702, 0.0010065862847797947, 0.0009967335373810354, 0.0009738742936405819, 0.0009877554571679369, 0.0009810573741977024, 0.00095753616415469, 0.0009540658789534078, 0.0009349469563681969, 0.0009096252078883044, 0.0009071082307982724, 0.0009211419426748822, 0.0009313682208376145, 0.0009372131938047127, 0.0009251861447081623, 0.0009259661095279513, 0.0009112957139827269, 0.0008976012554789287, 0.0008853185746509981, 0.0008723283998869408, 0.0008633347435569586, 0.0008567477123622252, 0.0008298316737417747, 0.0008122462321796907, 0.0007823578633625758, 0.0007643558419622175, 0.0007415542332238612, 0.0007257765157900834, 0.0007089918267770028, 0.0006909977550979701, 0.0006875067072638324, 0.0006897024401766254, 0.0006710023782651323, 0.0006826498481824718, 0.0006826536972080517, 0.0006758368163942333, 0.0006891202145408789, 0.0006987031963766121, 0.0007067874516746596, 0.0007020371933386629, 0.0007143980399370586, 0.0007233868487147299, 0.0007437278041890978, 0.0007682859937767673, 0.0008004641825176155, 0.0008326049409723723, 0.0008536253580080916, 0.0008895658708543191, 0.0009107798374619961, 0.0009341222384592379, 0.0009596702140766616, 0.0009686018898241179, 0.0009877780025201155, 0.0009942642260618367, 0.0009946168346178007, 0.0009938889918352704, 0.0009878082237524993, 0.0009948710736896545, 0.0009762740548296364, 0.0009745127608772966, 0.0009768583040419875, 0.0009617705806498426, 0.0009704158281336149, 0.0009981714715655888, 0.0010143263457399175, 0.001019430189408446, 0.0010332448362481528, 0.0010599774137917638, 0.0010765931965513002, 0.0011286540777895057, 0.001153686543527905, 0.0011509949469444135, 0.0011424964560199414, 0.0010781527056209562, 0.0010544993186938807, 0.0010245741939876624, 0.0009732485237187731, 0.0009559497642109997, 0.000923890696306972, 0.0009161017883664802, 0.0009041187169692107, 0.0009132632637044135, 0.0009192687010547865, 0.000933330140923415, 0.0009565453379868488, 0.0010012770101263497, 0.0010394172471161623, 0.001068536933932636, 0.0010894414203038408, 0.0011265327202095504, 0.0012201433981541018, 0.0012801098326910378, 0.001340041569986481, 0.0014420500287840754, 0.0015228360351890269, 0.0015821927940737291, 0.0016777537910448858, 0.001753555188171649, 0.001834172375068066, 0.0019030259087514966, 0.001990144908169046, 0.002101481501114712, 0.002152611725316387, 0.0022679324401559484, 0.002307970401605657, 0.0024082077157633187, 0.002512502825508773, 0.002593666637782024, 0.002679685573691809, 0.002775740725167584, 0.002909946529176581, 0.0030032670170628897, 0.0031137614241339225, 0.003245719897145068, 0.0033111998820296164, 0.0033883816415300255, 0.0034397337254983584, 0.003525948673690986, 0.003565085059272975, 0.003612119001999232, 0.003610261124494391, 0.0037276136641527205, 0.0037630239401475626, 0.003819131450412011, 0.00392060684161504, 0.00401259306902269, 0.004089383409532997, 0.00411800745364906, 0.004220954451788179, 0.0042860247336670085, 0.004339310428036243, 0.004391979195960898, 0.004381857746948711, 0.004375248541271957, 0.004307856413916116, 0.004299829827234907, 0.00423580076991775, 0.004136193057226318, 0.004052604367681877, 0.003946638972455931, 0.0038105186117954382, 0.0037672460888271775, 0.003657255074052614, 0.003546397844154737, 0.003435911471796755, 0.0033185985105642275, 0.0032288030559180195, 0.0031225145722861792, 0.003019076573115556, 0.002966403849950127, 0.002851732493431425, 0.002719975478535053, 0.0026090370498354984, 0.0025308101488148264, 0.002405594866961217, 0.0023064307944008627, 0.0022017375938391814, 0.0021041701925192777, 0.0020124364792846417, 0.001896190508553962, 0.0018215588551297808, 0.0017440474405516367, 0.0016845426263062413, 0.0016362936679095285, 0.0015996479203567986, 0.001555939539557324, 0.0015355875322275963, 0.0014997651351529187, 0.0014486172731512476, 0.0014260369448024356, 0.0014275046216691671, 0.0014081943131280259, 0.0014145671549247636, 0.0014386605870093459, 0.0014456073513041967, 0.0014830912513299958, 0.0015053241422353513, 0.0015609233491380436, 0.0016210741498606767, 0.0017050431267114587, 0.0017505666311964162, 0.001801786355098933, 0.0018736523769225773, 0.0019342880792736633, 0.001981313412400205, 0.0020825992235427747, 0.0021871426924025225, 0.0022735218859075736, 0.0023482085899322537, 0.002444822553684287, 0.0025436503780721653, 0.00264895247562673, 0.0027646034602603255, 0.0028874205180887185, 0.0030286149483379882, 0.003163336700844341, 0.0032966595070609713, 0.003400241743478041, 0.003500979145328706, 0.0035781632059745322, 0.0036876113940862054, 0.0035951473651125473, 0.0036512029416283435, 0.0035575205251410324, 0.0034832052924368352, 0.003333743041107985, 0.003196293702461143, 0.003132812585810398, 0.0030580553243174657, 0.0029788562127332505, 0.0028811242289862263, 0.002822334531372257, 0.0027942922509255667, 0.0027196554257833353, 0.002670686688719197, 0.002647792338562933, 0.0026930645977517346, 0.0026291420142221878, 0.0026209512684590044, 0.0025395970950572054, 0.0024929298612365643, 0.00248063425780063, 0.0024965680844730343, 0.002494697198948302, 0.002510846339423498, 0.002521144115546065, 0.0025684123140392113, 0.002610517418658968, 0.0026643451924869776, 0.002687068740507735, 0.002709032509206608, 0.002727782940835927, 0.0027036846026020724, 0.0026631918847061396, 0.0026422733863471597, 0.00261699174686029, 0.0026142474262537225, 0.002580207730466029, 0.002566742797432933, 0.0025846244462451474, 0.002571454048699854, 0.002608051134211458, 0.00266432794782711, 0.0027364523671255447, 0.0027680688226053196, 0.0028514040954976927, 0.002891359663248139, 0.0029529757693572078, 0.0030386319209927703, 0.0031079785184337074, 0.003221936050527758, 0.0033285166854447137, 0.003434956909176824, 0.0035529331435245285, 0.0036574931255850367, 0.0037288897516780666, 0.0037568863700687214, 0.0038319217761281473, 0.0038680895438576086, 0.003836778249261474, 0.0037452521238766673, 0.0036549729533894874, 0.0035389211836700865, 0.003409727317605129, 0.003277568573292804, 0.0031915929604877705, 0.0029963816310151817, 0.0028866244523110957, 0.0027643728483903586, 0.0026565470334389578, 0.0025521319438806915, 0.002469901582636379, 0.0024077314965152527, 0.002356064486388608, 0.0023396572958909547, 0.002305421113145578, 0.0023041610003412673, 0.0023099130901380665, 0.0023396550614478368, 0.0024261776401247846, 0.0025578227049798783, 0.00263979301231281, 0.0027014163957130447, 0.002769741193132716, 0.002829296099246656, 0.0028789388345034756, 0.0028732801836653925, 0.00286860930052705, 0.0028444040342933425, 0.002822890239961899, 0.0028734938432703667, 0.0028804805180259433, 0.0029365265488078777, 0.00298640535502027, 0.00309317018418469, 0.0031913334699393337, 0.00321909207252783, 0.003304703413905045, 0.0033893460219382014, 0.003504295449210059, 0.0036617296528599413, 0.0037925745482815737, 0.003871371397803517, 0.003963249042599188, 0.00400503662269383, 0.003977599369070423, 0.0038636963320044077, 0.0037508531169507028, 0.003605410133975045, 0.003438875115566212, 0.0032374827613794958, 0.0030413568045046355, 0.0028329007161127778, 0.0026371798384729073, 0.0024010183515117043, 0.002186141128814821, 0.001987690691648013, 0.0018284931322829802, 0.0016377407262989857, 0.0014707039162873048, 0.0013162498513982317, 0.0011742947232321916, 0.0010433518358924899, 0.0009219629304366581, 0.0008233909315290863, 0.0007439515284744198, 0.0006676522093040764, 0.000600171739753736, 0.000542572470098476, 0.000491472887214802, 0.0004449856658428946, 0.0004096672274211685, 0.00038063722452512817, 0.0003557478082805575, 0.00033592832878883864, 0.00031207871345988815, 0.00030095022973901804, 0.0002823448426754999, 0.00027328712778011487, 0.0002680014981008657, 0.00026271886883421866, 0.0002650776092387372, 0.0002663999723260958, 0.00027038610634020716, 0.00026950708988965577, 0.0002811156241015107, 0.0002961764898635711, 0.0003107006626911938, 0.00032696794977993656, 0.0003560830554100993, 0.0003805704396005649, 0.0004025840706460962, 0.0004257075819190978, 0.0004479002670815677, 0.0004815541575154473, 0.0005085953693499149, 0.0005359794287209537, 0.0005661400036472413, 0.0006044545908118166, 0.0006526221463805644, 0.0006989745141191866, 0.0007464097195918209, 0.0007869495725049483, 0.0008209358254893386, 0.0008460632994268019, 0.0008714432832758897, 0.0008760683674530939, 0.000877245294230456, 0.0008424095455044783, 0.0007938249660480274, 0.000735061261573895, 0.0006815538815512524, 0.0006228372011079824, 0.0005636655235029623, 0.0005170560525503407, 0.00046923536611273104, 0.0004323610534561823, 0.00039152823063340943, 0.00036578572100487867, 0.00033375217888973423, 0.00031297680183240507, 0.00029216791593610314, 0.0002697978816572821, 0.0002474479291651486, 0.00022589190959917748, 0.0002073195125195658, 0.00019332444081327418, 0.00017932914166275517, 0.00016653720836837778, 0.00015383640117245406, 0.00014216883928286274, 0.00013679948278113677, 0.0001326652630750419, 0.00012802823351062298, 0.00011721397328066883, 0.00011056434942482198, 0.00010440648753010651, 9.893162853067594e-05, 9.582711730385607e-05, 9.066141869391108e-05, 8.821130611649668e-05, 8.383530597485407e-05, 8.106929160375023e-05, 7.721695375674964e-05, 7.814331201605615e-05, 7.932865670634369e-05, 8.127140754145046e-05, 8.609721071259631e-05, 9.230187785648237e-05, 9.961118779671716e-05, 0.00010827885878952403, 0.00011622993303087267, 0.00012936811827943544, 0.0001392007024851317, 0.00015833363547632594, 0.00017473452559233223, 0.0001989436198930883, 0.0002201239034094251, 0.0002445099201659322, 0.0002739530775667998, 0.00030625982687340267, 0.0003403245988190286, 0.00037887452273560054, 0.00041690918964280025, 0.00046588191150994973, 0.0005209792543300221, 0.0005914333203764887, 0.0006625612747672067, 0.0007492964548880837, 0.0008436578155359381, 0.0009469099186193686, 0.0010562021681789378, 0.001161734177987971, 0.0012628317784753045, 0.001349672041611598, 0.0014116901562745217, 0.001443061233518748, 0.0014382772585113267, 0.001390649606667505, 0.0013224770158073952, 0.0012097307988581621, 0.0010904710271832864, 0.0009748184668980404, 0.0008573458786835184, 0.0007565692833329486, 0.0006712447439282175, 0.000598791487699679, 0.0005332411493225315, 0.00048117605873304245, 0.00044290092459547134, 0.0004014966345651113, 0.00036419204865937986, 0.00032972876269266103, 0.00030117340452901696, 0.00027185546167617905, 0.0002452683279699645, 0.00021753301414384599, 0.00019480099214217432, 0.00017305706380554338, 0.00015579812341367328, 0.00013767308702922846, 0.00012343646818650371, 0.0001091465215650572, 9.754240021371072e-05, 8.709697579966235e-05, 7.666851403538722e-05, 6.6951364854178e-05, 5.924386943030629e-05, 5.173998201058603e-05, 4.537545075714629e-05, 3.966537333877671e-05, 3.4268154958198425e-05, 3.3202315364474936e-05, 2.9219613445051476e-05, 2.74623120982156e-05, 2.4467723137799097e-05, 2.662538155637492e-05, 2.217455853633966e-05, 2.8564747607835142e-05, 8.879161580028141e-06, 3.518693171253185e-05, 2.1134284563819595e-05, 2.0846403189676182e-05, 1.921309979358777e-05, 1.3884924843488967e-05, 1.5452735314714557e-05, 1.546760890075513e-05, 2.7765084891913787e-05, 1.5784345849947026e-05, 2.1219592956282942e-05, 1.096672674337002e-05, 1.797958834428943e-05, 8.998647601548153e-06, 1.4617882472253967e-05, 1.5448605727629684e-05, 6.85115332119613e-06, 1.2404945424586168e-05, 9.117957285638796e-06, 1.6710725112585533e-05, 1.6804287175853553e-05, 1.6729615222788487e-05, 1.527591574959263e-05, 1.2854245143543314e-05, 6.755245525430734e-06, 2.018222445639348e-05, 2.2730031368084997e-05, 2.269501531126658e-05, 2.2682182404328204e-05, 3.279253857562138e-05, 3.2507521596378914e-05, 5.0049642779772464e-05, 5.6062992491905026e-05, 9.713815939999238e-06, 6.0582111261634526e-05, 6.374463292301277e-05, 2.1255502130762597e-05, 6.688990578270438e-05, 5.7791746647430064e-05, 1.276924219029841e-05, 6.329956175935169e-05, 5.443662098045291e-05, 1.755572567224726e-05, 5.70351288502003e-05, 4.540918962887233e-05, 5.6268023672866046e-05, 3.9823301487299205e-05, 4.036289598191928e-05, 4.0018499763987215e-05, 3.429849761643439e-05, 3.654568150341373e-05, 1.696688103374205e-05, 3.606354172437265e-05, 3.757431810288422e-05, 4.038573175953369e-05, 3.590220511152128e-05, 4.0293957051894246e-05, 3.814474663002695e-05, 3.4278388259499874e-05, 2.6876526827173935e-05, 3.144636083219744e-05, 3.0080124711991934e-05, 1.5409156379943172e-05, 2.5384573611931082e-05, 2.3987971120887687e-05, 1.6578187406545167e-05, 2.0405084057497364e-05, 2.068624532718681e-05, 1.8144724331658168e-05, 1.8761304230517615e-05, 1.909654394810706e-05, 2.2219548832102207e-05, 1.7405411194143734e-05, 1.7309412870965718e-05, 1.591277729945135e-05, 1.716065800102317e-05, 1.835097478050302e-05, 1.7289578181646498e-05, 1.5313544334757997e-05, 2.00965846317235e-05, 1.819384933330548e-05, 1.6777636464013426e-05, 2.0337059564201382e-05, 1.5350661453455865e-05, 1.6054967236608538e-05, 1.2716323445936118e-05, 1.0169806104060073e-05, 8.360360796525609e-06, 8.24747169010745e-06, 5.789896786462843e-06, 5.523902464939277e-06, 3.2936354287641693e-06, 2.7261180858392033e-06, 2.4487224932443704e-06, 2.1848130138741997e-06, 1.4764396255700735e-06, 1.13638790692297e-06, 7.229272759782978e-07, 3.832643009202149e-07, 3.5669638575163213e-07, 7.840796913773656e-07, 6.237785201660484e-07, 9.64273995429919e-07, 1.0087200065164768e-06, 1.209417620014892e-06, 1.983605793697513e-06, 3.581507060814965e-06, 4.817544418929917e-06, 7.335611160328796e-06, 8.5599926334962e-06, 1.187571482416111e-05, 1.0715375430607853e-05, 1.1627057470482546e-05, 1.4701443473172239e-05, 1.583469433604428e-05, 1.8964351946708305e-05, 2.2724261382313615e-05, 2.0446351526812552e-05, 2.1894360194691928e-05, 2.3800717704599404e-05, 2.3078524660028545e-05, 2.4837642803162898e-05, 2.4150521512034198e-05, 2.18911082262027e-05, 2.2162376714526373e-05, 2.1104648763430652e-05, 1.8310264159780295e-05, 2.020119707017262e-05, 1.467296416523329e-05, 1.367091463494803e-05, 1.4497283614111138e-05, 1.3328026440820353e-05, 1.2233862465274525e-05, 8.379371127565436e-06, 5.740501593094135e-06, 7.11064174032992e-06, 2.2482506549832075e-06, 2.8786534390079463e-06, 2.3033447030679758e-06, 1.2588265114066335e-06, 8.539656392765469e-07, 1.6761565945808662e-06, 1.0403028996095355e-06, 8.601434591404808e-07, 1.5506748821690654e-06, 1.3322072222946233e-06, 4.306319251706608e-07, 1.112872298445419e-06, 1.1903693879702738e-06, 6.110090639538278e-07, 1.3025690855328034e-06, 9.982425965062525e-07, 8.255416058616834e-07, 2.493490664274285e-06, 3.893166141840196e-06, 7.314907201361079e-06, 7.877793565242987e-06, 1.1066280121200675e-05, 1.2112510424420012e-05, 1.464495942001771e-05, 1.5251359200849187e-05, 1.2880372972194748e-05, 1.2706419455209876e-05, 1.2461873320526433e-05, 1.3477445387321365e-05, 1.5733701590984134e-05, 1.7185864479037382e-05, 3.127078572320373e-05, 2.043987263131491e-05, 1.7015925072288383e-05, 2.3984856853728886e-05, 2.7444251616125996e-05, 1.3883450417898885e-05, 3.4439152814318416e-05, 4.055242396968922e-05, 3.654358699910597e-05, 3.849450633668537e-05, 3.8162863540442723e-05, 3.540728237378009e-05, 2.820892097665308e-05, 3.5967392272183286e-06, 4.6125894064944586e-05, 3.9880070634938434e-05, 3.7127384529599337e-05, 3.532304328506183e-05, 2.429175985230943e-05, 2.1460811521054208e-05, 1.9348429936296642e-05, 1.805201585560995e-05, 4.125334844611391e-06, 7.50259841542254e-06, 3.091443054522126e-06, 2.322676614215847e-06, 1.3090694194186357e-06, 1.706669346276393e-06, 1.3212642269110733e-06, 6.909566551933539e-07, 2.3219723605480984e-07, 3.954563877453252e-07, 2.1493276425290033e-08, 2.3874165796579373e-09, 1.6167804191203103e-08, 2.7088389893492224e-08, 2.5617081771238957e-08, 1.0660030967342768e-06, 2.680164986491451e-06, 4.9741230677671055e-06, 1.2687092525788842e-05, 2.3567300690504967e-06, 1.3533233948595067e-05, 9.243951968209427e-06, 1.5245367465883252e-05, 1.3995737115496273e-05, 1.3822323668037856e-05, 1.809439141234032e-05, 4.576175304781343e-06, 7.131259631267531e-06, 7.545849485764704e-07, 3.657815094555781e-07, 1.4695898183871025e-06, 6.278404509134211e-07, 1.2035586418888224e-07, 1.5515685572047124e-07, 2.6561115645740228e-08, 4.4260104283400754e-08, 1.0290517422621058e-07, 7.587103927676766e-08, 2.402187153248224e-07, 1.5129421412466852e-07, 1.325058747077609e-07, 8.747155982749346e-07, 5.095720485201292e-07, 9.235150263749967e-06, 1.114497410262603e-06, 1.0080814533918584e-05, 3.13469970285011e-05, 3.5658633332185507e-05, 4.348586749317157e-05, 6.0342198262264264e-05, 5.732396711184506e-05, 4.541981113962893e-05, 3.059787763456758e-05, 2.9162316054530694e-05, 3.225089479529502e-05, 9.852001391108291e-06, 2.0664710622903704e-06, 5.338014575783132e-07, 1.5797278063110527e-06, 1.249541386094781e-06, 3.5243600234298397e-07, 1.611461324203185e-06, 9.233344578854672e-08, 1.8547225323708758e-05, 1.7529522937583462e-05, 7.484136035053591e-05, 3.391545828945636e-05, 4.9025114816545254e-05, 4.54147383247562e-05, 3.513957655164343e-05, 0.00012075346264252912, 6.713147818596192e-05, 3.872594696724383e-05, 3.0343920826764964e-05, 5.095575472938151e-05, 1.3662968841635369e-05, 4.178316285990939e-06, 2.1268752510301593e-05, 1.5633612901064276e-05, 1.194137798015952e-05, 1.2203653777486666e-05, 2.1860171807261293e-05, 1.9975713025552434e-05, 1.3683336333925045e-06, 1.1916268396778609e-05, 2.3951624167980184e-05, 2.4228919885592946e-05, 2.1146250503067475e-06, 3.611521690004031e-05, 3.097319501545567e-05, 4.644938685399057e-05, 1.9115198533824515e-05, 5.586220272710101e-05, 7.516560127610424e-05, 5.9166296566510555e-05, 6.195837289126766e-05, 7.613088478549474e-05, 0.00013756878750293701, 4.46373138522802e-05, 4.384998027955841e-05, 9.543471772198586e-05, 0.00013936677970122257, 9.05124852773436e-05, 9.050547438404184e-05, 9.100151271271747e-05, 0.00012077647969028218, 0.00011337830371708167, 0.00012261737290524148, 0.00010264089707550618, 0.00010622023194726136, 5.0124460819743316e-05, 0.00011027257303370297, 8.619274313391548e-05, 5.21231043654664e-05, 6.0105385634556604e-05, 8.081834941581257e-05, 5.488653528106507e-05, 5.163137959832652e-05, 7.098371375796513e-05, 2.6968854231109056e-05, 3.4858609578841184e-05, 4.666480673545587e-05, 4.1335262141123685e-05, 4.2060321714270113e-10, 3.586277823478592e-05, 3.53260253225921e-05, 3.626897996958321e-05, 3.408244011876498e-05, 3.492168734198229e-05, 2.9838061674104112e-05, 4.020378027649097e-05, 3.4383173711413275e-05, 1.822742199788405e-05, 3.101774729229532e-05, 3.933030852473145e-05, 3.36824269042507e-05, 5.042777354077493e-05, 3.636064380560667e-05, 3.242574059012413e-05, 3.0225027896637633e-05, 2.9723714287327426e-05, 1.6755667805877926e-05, 9.736823309395722e-06, 2.4543081903387005e-05, 2.733290860095035e-05, 2.6054373998065297e-05, 2.8062623035489624e-05, 2.4362065326918457e-05, 2.444141194581299e-05, 1.903168643775161e-05, 1.7147175160462012e-05, 2.8418882240744346e-06, 1.701099877473354e-05, 1.7165932104781637e-05, 1.3831320861625853e-05, 1.1315524977457838e-05, 9.23319514908596e-06, 1.0899450622804745e-05, 9.292895104538806e-06, 1.3796030771474379e-06, 3.513771772423551e-06, 6.787998977608977e-06, 1.1720626337379049e-05, 5.775092912528744e-06, 3.5198923746939517e-06, 3.536700328665298e-06, 3.6919189045704277e-06, 3.0744429156030283e-06, 1.3579363887252636e-09, 2.6493190779349976e-06, 2.410024040290937e-06, 1.9142871242839614e-06, 2.5418453869510677e-06, 2.0857029547098692e-06, 1.944818669501017e-06, 2.0273365950590093e-06, 2.0567973655778853e-06, 3.3922328493917916e-06, 2.530572676433121e-06, 3.033648218181734e-06, 3.3057355041052773e-06, 4.285636334852928e-06, 4.3339651859653784e-06, 4.6428216249292175e-06, 1.8776560222633173e-06, 2.4499938825811026e-06, 7.875266214923646e-06, 1.0192605888519366e-05, 8.497257733962844e-06, 1.0699722728414074e-05, 1.4419950441834285e-05, 1.0923603164359308e-05, 8.41225099132938e-06, 1.1783332683012488e-05, 1.8052667805562598e-05, 1.9858223942695688e-05, 2.496044923574308e-05, 3.135350136283797e-05, 3.006854305716308e-05, 2.3226128426471264e-05, 8.068444021101744e-08, 2.2692013373447812e-05, 2.623653685348577e-05, 3.071782279913731e-05, 2.481368668712429e-05, 2.5997616480833557e-05, 2.896663471621575e-05, 2.0317298941556804e-05, 2.4835125176886182e-05, 3.837486055801058e-05, 3.770799979123537e-05, 4.128834437987014e-05, 4.423294525890139e-05, 4.045844701473781e-05, 2.9180898800963352e-05, 2.9393609712807262e-05, 4.2201345919115866e-05, 3.5572850783998323e-05, 3.9363233364048655e-05, 4.820708458004684e-05, 3.84895169008439e-05, 2.9229818660534946e-05, 3.7183933793451823e-05, 2.9900792626725127e-05, 3.919125581999458e-05, 4.3744569963204054e-05, 4.3869346416507096e-05, 3.9128368846959644e-05, 5.58435513405166e-06, 8.004233268087589e-05, 4.178561577766591e-05, 4.266865212074125e-05, 4.484470992840848e-05, 4.401883266489729e-05, 3.542829719019897e-05, 2.560451419587411e-05, 3.81350080639885e-05, 4.0724646267435825e-05, 4.2706202630130523e-05, 3.2879290007644243e-05, 2.777373854515074e-05, 2.43410075521774e-05, 3.9251806160682054e-05, 2.4902652792455257e-05, 1.8939700549219108e-05, 1.7278505520346022e-05, 2.5096880669703512e-05, 1.0841189958459508e-05, 9.4089454734833e-06, 9.263075488786201e-06, 8.397014555906735e-06, 5.7674915476935355e-06, 5.308495935548618e-06, 4.852829391151106e-06, 5.991459978950865e-06, 5.467756149912084e-06, 5.865006449968688e-06, 5.9646364469055205e-06, 6.708586894058602e-06, 8.40175028410762e-06, 6.462074646257053e-06, 9.782887036794115e-06, 1.0774885791735006e-05, 1.1431963103373251e-05, 1.2912380799357166e-05, 1.4187268257725901e-05, 1.6240162837508008e-05, 1.6454190017253877e-05, 1.988792516092459e-05, 2.1365381383693757e-05, 2.327207202952146e-05, 2.431940612788885e-05, 2.2906820172540137e-05, 2.1952712080895526e-05, 2.729760186495016e-05, 2.7485342434179362e-05, 2.6580948334675182e-05, 2.4943063165460253e-05, 2.3084795211145692e-05, 2.174565721977121e-05, 2.0128658150840435e-05, 1.9607759849738726e-05, 1.9236475465104456e-05, 1.8899517604142158e-05, 1.877618040539045e-05, 1.8750006884229693e-05, 1.8957748954807113e-05, 1.6900887576587417e-05, 1.6974899784476213e-05, 1.658186545452359e-05, 1.7255901749618627e-05, 1.6894421036914065e-05, 1.67586512046122e-05, 1.6906354980180677e-05, 1.7215099988788645e-05, 1.8016820175312203e-05, 1.5206514699837809e-05, 1.811453188983454e-05, 1.943558084631242e-05, 2.003955407060578e-05, 2.2427346982918103e-05, 2.1937351281752825e-05, 2.31371611733833e-05, 2.6810510131065437e-05, 2.8609361487492094e-05, 3.5431259814670104e-05, 4.139492671102058e-05, 4.5967949537055655e-05, 4.668342919578713e-05, 5.650355769082582e-05, 6.188066314029745e-05, 7.196074143423372e-05, 8.045254588057591e-05, 8.84862334042998e-05, 9.65720550163903e-05, 0.00010802675223166968, 0.00011872506023107273, 0.00013003529161247454, 0.0001419813556915763, 0.00014984232733094918, 0.00015940019244741773, 0.0001601927183457613, 0.00016899557765863693, 0.00017217676579451164, 0.0001784715319770202, 0.00018489364920002286, 0.00018807567525600175, 0.00019489994837094992, 0.00019762095442357652, 0.00019831711130108387, 0.00020278910881942495, 0.00020881522151195795, 0.00021852011655223763, 0.000228849916574502, 0.00024093736249945657, 0.00025540368469624444, 0.0002687680540766995, 0.0002813532769750584, 0.00029796200638972645, 0.00031090802775716954, 0.00032043902308225583, 0.0003311764304248954, 0.0003362354549769188, 0.00033679937558112494, 0.00033944599230952596, 0.00033189710632189643, 0.0003229003180191793, 0.00031519714996622674, 0.00030089984552549465, 0.00028797308504847083, 0.0002741636869940872, 0.0002567726085666747, 0.0002398784964425973, 0.00022453621358981725, 0.0002137376306658527, 0.000199452538792954, 0.0001967925562615392, 0.000197968494053626, 0.00020080699429269423, 0.0002051273477713077, 0.00021140379568144145, 0.00021415818342468234, 0.0002171929906206225, 0.00022264604425441343, 0.00022792838503319252, 0.00023513471032884318, 0.0002426510027031627, 0.0002483649866057864, 0.00025505379504531597, 0.0002599667348675177, 0.00026315467689356917, 0.00026763504446650373, 0.0002651206715365705, 0.00026213369231320926, 0.00026030727501457805, 0.0002565935008603912, 0.00025742377962768116, 0.0002475722806449776, 0.00024365625834102598, 0.0002446726311386289, 0.00024478753401258534, 0.0002488237567362235, 0.0002521381864802859, 0.00025555493213628175, 0.0002622775812989194, 0.0002662222627830671, 0.0002792625482582771, 0.0002982261524434868, 0.000315030487499045, 0.00033024097435919547, 0.0003428282004901831, 0.0003497017977861665, 0.00036236682303049747, 0.00035976179387921963, 0.0003567065810682153, 0.0003541774945296458, 0.0003500925015610035, 0.0003584469884163895, 0.0003764611980420391, 0.0003814261834804967, 0.0003927375845792055, 0.00038775026943547514, 0.0004034007842793015, 0.00041601957000910844, 0.00043639375996981194, 0.0004600784136284138, 0.0004454792772411992, 0.00040565005032727687, 0.00039588436386157904, 0.0003907787883130849, 0.00038860338914209063, 0.0003841162698673565, 0.0003835100664020522, 0.00037910107577789114, 0.00037910820142527415, 0.0003811826804668265, 0.0003858415548581496, 0.00039435092419744076, 0.00039807605241173676, 0.00040819965567868673, 0.00041486843549248706, 0.0004337369428400937, 0.00042870081096263793, 0.00043512021308177623, 0.0004359242867215871, 0.0004424542095146695, 0.00046306690622414264, 0.0004555194857117611, 0.0004555686653919482, 0.00045997939815350675, 0.0004784293809124698, 0.00047435945011621556, 0.0004764450975045599, 0.00048538782659670975, 0.0004936162965512723, 0.0005075750211575834, 0.0005138938677763138, 0.0005221973633627509, 0.0005289139765694568, 0.000531017675145187, 0.0005954137484429416, 0.0005362823676012861, 0.0005345814914216039, 0.0005263061338818332, 0.0005206944985961349, 0.0005090486803462196, 0.00048350030050811095, 0.0004668406630490904, 0.00044117071297136696, 0.00043827415163673035, 0.00040787234201186874, 0.0003866325455229077, 0.00036677699117309713, 0.0003505660697956644, 0.0003465301956998549, 0.00031612761770757197, 0.00029544695714071856, 0.0002895328966890267, 0.00028061948447580384, 0.00027396204644665576, 0.00026668689111746083, 0.0002668802686605533, 0.00025415803577513745, 0.00024845921213356107, 0.00024663336458377615, 0.0002439875739218207, 0.00024288914614906343, 0.0002704814505952757, 0.00023867503602203528, 0.0002276611554888396, 0.0002156915517832458, 0.00021739375071603417, 0.000213104858391656, 0.00020106134840878375, 0.00019820969486095618, 0.0001970830579941002, 0.00018969053826681335, 0.00018318704554412361, 0.00017879227540279938, 0.00017831610565721155, 0.0001795861518778685, 0.0001698681363181483, 0.00016179207864449924, 0.00014248637405095906, 0.0001268067177713396, 0.00012302948818935987, 0.00012108945998991853, 0.00011935199612135828, 0.00012479389143082458, 0.00010750580779969773, 0.00010785913337730017, 0.00010442528552947951, 0.00010578025895095136, 9.925259290428068e-05, 9.601148390569142e-05, 9.374035481334047e-05, 9.388019614800486e-05, 8.96973933963365e-05, 8.757487320482897e-05, 8.414744602523777e-05, 8.108061114047708e-05, 7.872608326607933e-05, 7.733023815638366e-05, 7.512619801986929e-05, 7.195987515627085e-05, 7.051506498482844e-05, 6.895683202273036e-05, 6.71463413470848e-05, 6.627919076929742e-05, 6.492539311278402e-05, 6.281307568202012e-05, 6.0361339042683624e-05, 6.066540789492122e-05, 5.83449522558387e-05, 5.6890805998193e-05, 5.511244189739589e-05, 5.2816447562691515e-05, 5.182368360676248e-05, 4.9970677914387675e-05, 5.13724125713645e-05, 4.9258476711747065e-05, 4.708553301934527e-05, 4.679484313012991e-05, 4.265630064300904e-05, 4.589625631751439e-05, 4.5461100702945613e-05, 4.3995202017992356e-05, 4.485447027291967e-05, 4.157554878188345e-05, 4.443529963033201e-05, 4.450570604753449e-05, 4.0005715226421e-05, 4.4774831097465915e-05, 4.588354314696735e-05, 4.589578489620753e-05, 4.68514813075806e-05, 4.244453141082619e-05, 4.966268205056146e-05, 5.0060268386834104e-05, 4.587342579271476e-05, 5.168753220142048e-05, 5.3248174931265066e-05, 5.262803210391175e-05, 5.7087681287849185e-05, 5.9926311661353106e-05, 5.995699084242137e-05, 6.241689809339274e-05, 6.349379625566665e-05, 6.665753396205767e-05, 6.792261390112567e-05, 6.983690010905518e-05, 7.440529478541302e-05, 7.614723017639266e-05, 7.81194075698199e-05, 7.677996556073193e-05, 8.176606760656907e-05, 8.209798137385425e-05, 8.285504887102816e-05, 9.069381023627217e-05, 9.38041435168681e-05, 8.667471734605702e-05, 0.00010016951009537246, 0.0001021012480416925, 0.00010369977771520843, 0.00010694085923104313, 0.00011424153765607535, 0.00011420744785468025, 0.0001040735223973879, 0.00011649657709576521, 0.0001147498950274991, 0.00012122235421382159, 0.0001093851494654329, 0.00012050644623343661, 0.00012129270295309739, 0.00012212498124823998, 0.0001153238764834863, 0.0001223679629042027, 0.00012186944616727733, 0.0001225173445510394, 0.00012361702818290476, 0.0001258531660625477, 0.00012602263195936875, 0.0001257810381482027, 0.00013044225084294934, 0.00012959408153677225, 0.00013016814915026118, 0.00012726974329519948, 0.00012661496107132744, 0.00012684504459397947, 0.00012774439884087027, 0.0001282365703604592, 0.0001274608070064064, 0.0001270191095645076, 0.00012908932874314204, 0.00012957859023162265, 0.00013222641567648031, 0.00013419045012709693, 0.0001370896339140406, 0.00013980668713184296, 0.0001421607645142864, 0.00014541392475892717, 0.00014765172863153404, 0.0001485934063669609, 0.00014797354134558422, 0.00014747553860327197, 0.00014609318663029553, 0.00014276600672133603, 0.00013949087538409462, 0.00013635009008902536, 0.00013103962419593333, 0.00012551422997298054, 0.00011965274709637611, 0.00011383061770223486, 0.00010691029586446311, 0.00010135860872588518, 9.744979469641854e-05, 9.240398491071238e-05, 8.827325017405202e-05, 8.391978866454356e-05, 8.288898560198895e-05, 8.00168671199309e-05, 8.058241646749142e-05, 7.766041208680084e-05, 7.788502854216316e-05, 7.32854752907681e-05, 7.037392553651742e-05, 6.784055581330458e-05, 6.641404460420366e-05, 6.289857590257382e-05, 5.9634087418978725e-05, 5.74924173344848e-05, 5.54846642178404e-05, 5.223931591763996e-05, 5.357205391893012e-05, 5.064497170998089e-05, 4.745542136022657e-05, 4.3785955237060664e-05, 4.2458178505853645e-05, 4.184328751940281e-05, 3.897178292329317e-05, 3.453369456130115e-05, 3.159968826812387e-05, 2.8795642291865616e-05, 2.7820532765062933e-05, 2.7576375559748334e-05, 2.748647749179483e-05, 2.6634787244444163e-05, 2.8110903264980065e-05, 2.9352166924539058e-05, 3.0007037100650737e-05, 3.06903754767152e-05, 3.15038813543734e-05, 3.326181418390311e-05, 3.5168064325819884e-05, 3.77144230871134e-05, 3.910402770751234e-05, 4.3312071141027613e-05, 4.9743868817561285e-05, 5.8654236664568e-05, 6.722036411110642e-05, 7.057539993295113e-05, 7.446611040177183e-05, 8.09416049137991e-05, 8.649753392173782e-05, 9.173367835628398e-05, 0.00010369392855921792, 0.00010878779707257037, 0.00011860551112500526, 0.0001233593079495819, 0.00012772321346747826, 0.00012794943706324652, 0.00013633090516961858, 0.00013846857430668849, 0.00013932261760798406, 0.00013589189123156006, 0.0001283678995335172, 0.00012265733325069877, 0.00012369376072286396, 0.0001281678069628114, 0.00013589169479437502, 0.00014329159457386362, 0.00015188865824495679, 0.00016038785355439837, 0.00016905291904507804, 0.00017885176989914248, 0.0001883934007812072, 0.0001974905275096068, 0.00020875971666277506, 0.0002187319505345271, 0.00023190273089059496, 0.00024897803932056646, 0.00026753845735570203, 0.00029177823983697696, 0.00031466166695021455, 0.0003340594776846526, 0.000360496913700272, 0.00038285094130797976, 0.0004023464520435184, 0.0004223026069832032, 0.00044711530558106776, 0.00047245329080501154, 0.0004977120380186015, 0.0005283095735521533, 0.0005622602505749327, 0.0005784408724715666, 0.0006025747196117772, 0.0006428422647031923, 0.0006800172074293239, 0.0007111715602132795, 0.0007486825951761429, 0.000794487677832894, 0.0008378693167632434, 0.0008770649316032674, 0.0009137611412703763, 0.0009401924161417192, 0.0009573869702590702, 0.0009745019811599, 0.0009906165560053567, 0.0010123025227625339, 0.001040556116264534, 0.0010616276064819108, 0.001077533813005996, 0.0010884014762773792, 0.0010932160400752048, 0.0010826350166750834, 0.001065896333300815, 0.0010446762011058956, 0.00102474599163488, 0.001001635106262904, 0.000965856581885443, 0.0009262338645897196, 0.0008854749264649268, 0.0008556579721067454, 0.000821575523513436, 0.0007811765365817485, 0.0007570832851855742, 0.0007221828196357503, 0.0007135249789520128, 0.0007095043493962884, 0.0006761751631029146, 0.0006549970115784546, 0.0006419106253787853, 0.0006309863694508218, 0.0006081088351027609, 0.0006060622120618937, 0.0005859391925410958, 0.0005682100578232319, 0.0005791514815712164, 0.0005654731758693186, 0.0005760517998135681, 0.0005464634221592061, 0.0005553916712246235, 0.0005547792580052787, 0.0005432269787122305, 0.0005430654830952494, 0.0005405893614488959, 0.0005489395951705425, 0.0005256613307241187, 0.0005259960914228526, 0.0005261639834607221, 0.0005208594422477721, 0.0005235907757652664, 0.0005076573345913972, 0.000527570613235307, 0.0005131834520090224, 0.0005278734273463354, 0.0005160183705611362, 0.0005136194394991146, 0.0005070374549236372, 0.0005114265248437896, 0.0005339316137097415, 0.000503865381312227, 0.0005022184707219123, 0.0005081321464736013, 0.0005214098745173162, 0.0005279448763169988, 0.0005109131850612618, 0.000518334842540079, 0.000517738187791463, 0.0005216809961943451, 0.0005281137560318671, 0.0005185808137785098, 0.0005190949643757559, 0.0005173171866959151, 0.0005408480037526764, 0.000515202403128589, 0.0005331657930729033, 0.000540315622946279, 0.0005204610355952115, 0.0005466640582797153, 0.0005206514157810185, 0.000542179632766528, 0.0005567849203618835, 0.0005335556578080372, 0.000528365778886283, 0.0005269918546795893, 0.0005348982734038627, 0.0005305972859116935, 0.0005287598701775476, 0.0005569158254599408, 0.0005639004284688497, 0.000568636612820012, 0.0005803635076423423, 0.0005495953724464977, 0.0005778648217810372, 0.0005453441934168087, 0.0005799210012249469, 0.0005430911752572997, 0.0005412243193648152, 0.0005463518641547604, 0.0006123659245344485, 0.000595002208236628, 0.0006107581394677414, 0.0005960855653605714, 0.0005547538440475979, 0.0006076010597906977, 0.0005881617789245471, 0.0005601474780115024, 0.0005990273389676721, 0.0005981040553188574, 0.0006399497548908846, 0.0006102884181150219, 0.0005882079955639289, 0.0005718912855850973, 0.0005699102015766193, 0.0006082396869955848, 0.0006186098037937739, 0.0005649907683705711, 0.0006045567398860901, 0.0005733484357707387, 0.0006249700391593057, 0.0006482500201423981, 0.0005930329993889876, 0.0006382536335717344, 0.0006138667742732762, 0.0006840805528580084, 0.0006715462694014083, 0.0006454419650045359, 0.0006821657888537123, 0.0006716964223795355, 0.0006110269632392416, 0.0007101869592139663, 0.0006938920056265282, 0.0006719922429994951, 0.0006998485101523828, 0.0007001899858992293, 0.0006057039008275911, 0.0006961141711005913, 0.0006222827458064857, 0.0007048448933258118, 0.0006620659197507201, 0.0006980812285113353, 0.0007429456909840625, 0.0007450737150287868, 0.0007427436264209422, 0.0007435003023916238, 0.0007489839472504072, 0.0007289996673332804, 0.0007135778630989781, 0.0007391276155529889, 0.0007483690695846292, 0.0007494070573950581, 0.0007667460593952219, 0.0007621339274942398, 0.0007590435625393366, 0.0007668988554528142, 0.00076835047947623, 0.0007638985647694376, 0.00077456618503518, 0.0007858298829367592, 0.0007913538502561421, 0.0008115842433715798, 0.0007536382058401061, 0.0007328622036250662, 0.0007327018866245072, 0.0007197720019127211, 0.0007361461606810804, 0.000737581480154556, 0.0007280441359420387, 0.0007294855925957462, 0.0007475719395069898, 0.0007511755500916195, 0.0007623982602862995, 0.000742723856395316, 0.0007560797680003187, 0.0007575902809298766, 0.0007781321177680588, 0.0007926275506064197, 0.0007741516174624881, 0.0007856834554737046, 0.0008033875858660105, 0.0007795800170580904, 0.0007819017745856485, 0.000776002469245532, 0.000770253194804204, 0.0007645411675744804, 0.0007635706404396236, 0.0007668572759364933]\n"
     ]
    }
   ],
   "source": [
    "X = padded_sequences \n",
    "spectra = []\n",
    "\n",
    "with open(\"computed_spectra.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) \n",
    "    for row in reader:\n",
    "        try:\n",
    "            spec = [float(v) for v in row[1:1802]] \n",
    "            if len(spec) == 1801:\n",
    "                spectra.append(spec)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "X = X[:len(spectra)]\n",
    "\n",
    "# create df\n",
    "df = pd.DataFrame({\n",
    "    \"encoded_smiles\": list(X),\n",
    "    \"spectra\": spectra\n",
    "})\n",
    "\n",
    "# testinggg\n",
    "print(\"First 10 encoded tokens:\", df.iloc[0]['encoded_smiles'][:10])\n",
    "print(\"First 10 spectra values:\", df.iloc[0]['spectra'][:10])\n",
    "\n",
    "print(\"Length of encoded_smiles vector:\", len(df.iloc[0]['encoded_smiles']))\n",
    "print(\"Length of spectra vector:\", len(df.iloc[0]['spectra']))\n",
    "\n",
    "print(\"Full encoded_smiles vector:\", df.iloc[0]['encoded_smiles'])\n",
    "print(\"Full spectra vector:\", df.iloc[0]['spectra'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INN Model:\n",
    "## Goal: SMILES embedding (X)  âŸ·  Spectra (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling Layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask # binary mask, (0/1) tensor of shape dim\n",
    "\n",
    "        # scale and translate are subnetworks used in RealNVP (are only applied to unmasked part of the input)\n",
    "        self.scale_net = nn.Sequential(nn.Linear(dim, hidden_dim),nn.ReLU(), nn.Linear(hidden_dim, dim),nn.Tanh())  \n",
    "        self.translate_net = nn.Sequential(nn.Linear(dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, dim))  \n",
    "\n",
    "    def forward(self, x): # Smiles -> Spectra\n",
    "        x_masked = x * self.mask # mask part of the input (x1)\n",
    "        s = self.scale_net(x_masked) * (1 - self.mask) # first part of the transformation (x2)\n",
    "        t = self.translate_net(x_masked) * (1 - self.mask) # second part of the transformation\n",
    "        y = x_masked + (1 - self.mask) * (x * torch.exp(s) + t) # combines to create y (go to spectra)\n",
    "        return y\n",
    "\n",
    "    def inverse(self, y): # computes the inverse (Spectra -> Smiles)\n",
    "        y_masked = y * self.mask # mask part of the input (x1)\n",
    "        s = self.scale_net(y_masked) * (1 - self.mask) # transformation 1\n",
    "        t = self.translate_net(y_masked) * (1 - self.mask) # transformation 2 \n",
    "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s)) # combines to create x (go back to smiles)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RealNVP Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, num_blocks=6): # num_blocks = number of coupling layers stacked\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_blocks):\n",
    "            mask = self.get_mask(i, dim)\n",
    "            self.layers.append(CouplingLayer(dim, hidden_dim, mask))\n",
    "\n",
    "    def get_mask(self, i, dim):\n",
    "        mask = torch.zeros(dim)\n",
    "        mask[i % 2::2] = 1\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x): # smile -> spectra\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def inverse(self, y): # spectra -> smile\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.inverse(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Try :)\n",
    "### Create Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2854, Test size: 714\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.X[idx], \u001b[38;5;28mself\u001b[39m.Y[idx]\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# train/test datasets\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m train_dataset = \u001b[43mSmilesSpectraDS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m test_dataset = SmilesSpectraDS(test_df)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# dataloaders\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mSmilesSpectraDS.__init__\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.X = torch.tensor(df[\u001b[33m'\u001b[39m\u001b[33mencoded_smiles\u001b[39m\u001b[33m'\u001b[39m].tolist(), dtype=torch.float32)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mself\u001b[39m.Y = torch.tensor(df[\u001b[33m'\u001b[39m\u001b[33mspectra\u001b[39m\u001b[33m'\u001b[39m].tolist(), dtype=torch.float32)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m X_train, X_test, y_train, y_test = train_test_split(X, \u001b[43my\u001b[49m, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Step 2: Custom Dataset class\n",
    "class SmilesSpectraDS(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.X = torch.tensor(df['encoded_smiles'].tolist(), dtype=torch.float32)\n",
    "        self.Y = torch.tensor(df['spectra'].tolist(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# Step 3: Datasets and Dataloaders\n",
    "train_dataset = SmilesSpectraDS(train_df)\n",
    "test_dataset = SmilesSpectraDS(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Step 4: Define model\n",
    "model = RealNVP(dim=1801, hidden_dim=512, num_blocks=6)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Step 5: Train model\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Step 6: Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        test_loss += loss.item()\n",
    "        y_true_all.append(y_batch)\n",
    "        y_pred_all.append(y_pred)\n",
    "\n",
    "y_true = torch.cat(y_true_all).numpy()\n",
    "y_pred = torch.cat(y_pred_all).numpy()\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nTest MSE: {mse:.4f}\")\n",
    "print(f\"Test RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "# Step 7: Plot one example\n",
    "x_test, y_test = next(iter(test_loader))\n",
    "y_pred = model(x_test)\n",
    "\n",
    "plt.plot(y_test[0].numpy(), label='True Spectrum')\n",
    "plt.plot(y_pred[0].detach().numpy(), label='Predicted Spectrum')\n",
    "plt.legend()\n",
    "plt.title(\"Test Example: True vs Predicted\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m model.eval()  \u001b[38;5;66;03m# evaluation mode\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     X_test_tensor = torch.tensor(\u001b[43mX_test\u001b[49m, dtype=torch.float32)\n\u001b[32m      5\u001b[39m     y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n\u001b[32m      6\u001b[39m     y_pred_tensor = model(X_test_tensor)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()  # evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "\n",
    "    y_true = y_test_tensor.numpy()\n",
    "    y_pred = y_pred_tensor.numpy()\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RÂ² Score: {r2:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
